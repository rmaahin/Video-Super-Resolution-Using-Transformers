{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bdaa13-0573-40e5-a951-b2d806938169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Video-Super-Resolution-Using-Transformers'...\n",
      "remote: Enumerating objects: 48743, done.\u001b[K\n",
      "remote: Total 48743 (delta 0), reused 0 (delta 0), pack-reused 48743 (from 1)\u001b[K\n",
      "Receiving objects: 100% (48743/48743), 1.71 GiB | 41.31 MiB/s, done.\n",
      "Updating files: 100% (48018/48018), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/rmaahin/Video-Super-Resolution-Using-Transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c5ec2b-8e62-402e-875f-0fd01b479755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m704.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cd7fa2-1ed4-4360-b6ac-038a629b2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import copy\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcd85b8-86ef-4add-9fc9-03899aa7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Video-Super-Resolution-Using-Transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac22c3b-ed8b-45db-8c6a-0d8b7507acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=5, scale=3, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root folder containing video subfolders.\n",
    "            sequence_length (int): Number of consecutive LR frames as input.\n",
    "            scale (int): Resolution upscaling factor.\n",
    "            mode (str): 'train' or 'test'. For 'train' random center, 'test' sliding window.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.half = sequence_length // 2\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.videos = []\n",
    "\n",
    "        self.samples = []\n",
    "        self.to_tensor = ToTensor()\n",
    "\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        video_folders = sorted(glob.glob(os.path.join(self.root_dir, \"*\")))\n",
    "\n",
    "        for vid_path in video_folders:\n",
    "            lr_folder = os.path.join(vid_path, \"lr_images\")\n",
    "            hr_folder = os.path.join(vid_path, \"hr_images\")\n",
    "            lr_frames = sorted(glob.glob(os.path.join(lr_folder, \"*.png\")))\n",
    "\n",
    "            if len(lr_frames) < self.sequence_length:\n",
    "                continue\n",
    "\n",
    "            for i in range(self.half, len(lr_frames) - self.half):\n",
    "                self.samples.append({\n",
    "                    \"video\": os.path.basename(vid_path),\n",
    "                    \"center_index\": i,\n",
    "                    \"lr_paths\": [os.path.join(lr_folder, f\"{idx:08d}.png\")\n",
    "                                 for idx in range(i - self.half, i + self.half + 1)],\n",
    "                    \"hr_path\": os.path.join(hr_folder, f\"{i:08d}.png\")\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        lr_seq = [self.to_tensor(Image.open(p)) for p in sample[\"lr_paths\"]]\n",
    "        hr_img = self.to_tensor(Image.open(sample[\"hr_path\"]))\n",
    "\n",
    "        lr_seq = torch.stack(lr_seq, dim=0)  # [T, C, H, W]\n",
    "        return {\n",
    "            \"lr\": lr_seq,                   # input sequence [T, 3, H, W]\n",
    "            \"hr\": hr_img,                   # target HR frame [3, H*scale, W*scale]\n",
    "            \"video\": sample[\"video\"],\n",
    "            \"index\": sample[\"center_index\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c96051-c6fc-418d-bc75-18d9af34c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STCSA(nn.Module):\n",
    "    def __init__(self, channels, patch_size=3, max_frames=7):  # <-- NEW\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.query_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.key_conv   = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.value_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.out_proj   = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.temporal_embed = nn.Parameter(torch.zeros(1, max_frames, channels, 1, 1))  # <-- NEW\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        if T != self.temporal_embed.shape[1]:\n",
    "            raise ValueError(f\"Temporal embed expects {self.temporal_embed.shape[1]} frames, got {T}\")\n",
    "\n",
    "        x = x + self.temporal_embed[:, :T]  # broadcast addition\n",
    "\n",
    "        x_reshape = x.view(B*T, C, H, W)\n",
    "        Q = self.query_conv(x_reshape)\n",
    "        K = self.key_conv(x_reshape)\n",
    "        V = self.value_conv(x_reshape)\n",
    "\n",
    "        Q = Q.view(B, T, C, H*W).permute(0, 2, 1, 3).reshape(B, C, -1)\n",
    "        K = K.view(B, T, C, H*W).permute(0, 2, 3, 1).reshape(B, C, -1)\n",
    "        V = V.view(B, T, C, H*W).permute(0, 2, 1, 3).reshape(B, C, -1)\n",
    "\n",
    "        attn_weights = torch.softmax(torch.bmm(Q.transpose(1,2), K) / (C ** 0.5), dim=-1)\n",
    "        out = torch.bmm(attn_weights, V.transpose(1,2))\n",
    "        out = out.transpose(1,2).reshape(B, C, T, H, W).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        out = self.out_proj(out.view(B*T, C, H, W)).view(B, T, C, H, W)\n",
    "        return x + out  # Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18066a4-818c-4367-8aab-0aa13caf5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(B, H, W, device):\n",
    "    y, x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\"ij\")\n",
    "    grid = torch.stack((x, y), 2).float()  # [H, W, 2]\n",
    "    grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)  # [B, H, W, 2]\n",
    "    return grid.to(device)\n",
    "\n",
    "def warp(x, flow):\n",
    "    \"\"\"\n",
    "    Warp an image or feature map with optical flow\n",
    "    x: [B, C, H, W]\n",
    "    flow: [B, 2, H, W] in pixels\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.size()\n",
    "    grid = generate_grid(B, H, W, x.device)  # [B, H, W, 2]\n",
    "    grid = grid + flow.permute(0, 2, 3, 1)  # add flow\n",
    "    grid[:, :, :, 0] = 2.0 * grid[:, :, :, 0] / (W - 1) - 1.0\n",
    "    grid[:, :, :, 1] = 2.0 * grid[:, :, :, 1] / (H - 1) - 1.0\n",
    "    return F.grid_sample(x, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "\n",
    "class FlowEstimator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, 2, 3, 1, 1)  # Output flow: 2 channels\n",
    "        )\n",
    "\n",
    "    def forward(self, ref, nbr):\n",
    "        x = torch.cat([ref, nbr], dim=1)\n",
    "        flow = self.encoder(x)\n",
    "        return flow\n",
    "\n",
    "class BOFF(nn.Module):\n",
    "    def __init__(self, channels, sequence_length): # Add sequence_length as argument\n",
    "        super().__init__()\n",
    "        self.flow_net = FlowEstimator(channels)\n",
    "        self.fuser = nn.Conv2d(channels * sequence_length, channels, 3, 1, 1) # Use sequence_length\n",
    "        self.norm = nn.LayerNorm([channels, 1, 1])\n",
    "        self.sequence_length = sequence_length # Store sequence_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.size()\n",
    "        center_idx = T // 2\n",
    "        ref = x[:, center_idx]  # [B, C, H, W]\n",
    "\n",
    "        warped_feats = [ref]  # include center frame unwarped\n",
    "\n",
    "        for i in range(T):\n",
    "            if i == center_idx:\n",
    "                continue\n",
    "            nbr = x[:, i]\n",
    "            flow = self.flow_net(ref, nbr)\n",
    "            warped = warp(nbr, flow)\n",
    "            warped_feats.append(warped)\n",
    "\n",
    "        aligned = torch.cat(warped_feats, dim=1)  # [B, C*T, H, W]\n",
    "        fused = self.fuser(aligned)  # [B, C, H, W]\n",
    "\n",
    "        # Expand to [B, T, C, H, W] with same fused output\n",
    "        return x + fused.unsqueeze(1).expand(-1, T, -1, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4326839-0a4a-411b-90ba-4b9e09128f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconstructor(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=3, scale=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * (scale ** 2), kernel_size=3, padding=1)\n",
    "        self.upsample = nn.PixelShuffle(scale)\n",
    "\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 32, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.refine(x)  # <--- extra sharpness refinement\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08d0742-cc1e-4098-8acb-20e658a05c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=3, feat_channels=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, feat_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feat_channels, feat_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.layers(x)  # apply sequential\n",
    "        return feats.view(B, T, -1, H, W)  # [B, T, C, H, W]\n",
    "\n",
    "class VSRTransformer(nn.Module):\n",
    "    def __init__(self, in_channels=3, feat_channels=64, scale=3, num_blocks=4, sequence_length=5): # Add sequence_length\n",
    "        super().__init__()\n",
    "        self.feat_extractor = FeatureExtractor(in_channels, feat_channels)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                STCSA(feat_channels, max_frames=sequence_length),\n",
    "                BOFF(feat_channels, sequence_length) # Pass sequence_length to BOFF\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.reconstructor = Reconstructor(feat_channels, in_channels, scale)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, 3, H, W]\n",
    "        feats = self.feat_extractor(x)  # [B, T, C, H, W]\n",
    "        for block in self.blocks:\n",
    "            feats = block(feats)\n",
    "        center = feats[:, feats.shape[1] // 2]  # [B, C, H, W]\n",
    "        sr = self.reconstructor(center)\n",
    "        return sr  # [B, 3, H*scale, W*scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62772ccd-69a0-4788-a131-d85c97ec9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        diff = pred - gt\n",
    "        loss = torch.mean(torch.sqrt(diff * diff + self.eps * self.eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0e8f95-353d-4407-a944-a18e7a68882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_psnr(sr, hr):\n",
    "    sr = torch.clamp(sr, 0.0, 1.0)\n",
    "    hr = torch.clamp(hr, 0.0, 1.0)\n",
    "    mse = torch.mean((sr - hr) ** 2)\n",
    "    if mse == 0:\n",
    "        return float(\"inf\")\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b5d9fa-102c-4b4c-add6-8e40ab31cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "405e4113-b9e3-40f4-a82b-eba6d810bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Train Loss: 0.0450\n",
      "[Epoch 1] Val Loss: 0.0439 | Val PSNR: 23.06 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Train Loss: 0.0362\n",
      "[Epoch 2] Val Loss: 0.0408 | Val PSNR: 23.52 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Train Loss: 0.0344\n",
      "[Epoch 3] Val Loss: 0.0404 | Val PSNR: 23.70 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Train Loss: 0.0334\n",
      "[Epoch 4] Val Loss: 0.0383 | Val PSNR: 23.93 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Train Loss: 0.0328\n",
      "[Epoch 5] Val Loss: 0.0374 | Val PSNR: 24.04 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Avg Train Loss: 0.0323\n",
      "[Epoch 6] Val Loss: 0.0371 | Val PSNR: 24.16 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Avg Train Loss: 0.0320\n",
      "[Epoch 7] Val Loss: 0.0368 | Val PSNR: 24.21 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Avg Train Loss: 0.0316\n",
      "[Epoch 8] Val Loss: 0.0369 | Val PSNR: 24.24 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Avg Train Loss: 0.0314\n",
      "[Epoch 9] Val Loss: 0.0363 | Val PSNR: 24.32 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Avg Train Loss: 0.0312\n",
      "[Epoch 10] Val Loss: 0.0359 | Val PSNR: 24.36 dB\n",
      "✅ Best model saved to vsr_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"data/config.yaml\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "dataset = VSRDataset(\n",
    "    root_dir=config[\"dataset_path\"],\n",
    "    sequence_length=config[\"sequence_length\"],\n",
    "    scale=config[\"scale\"]\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "\n",
    "# Model\n",
    "model = VSRTransformer(\n",
    "    in_channels=3,\n",
    "    feat_channels=config[\"feat_channels\"],\n",
    "    scale=config[\"scale\"],\n",
    "    num_blocks=config[\"num_blocks\"],\n",
    "    sequence_length=config[\"sequence_length\"]\n",
    ").to(device)\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = VSRDataset(\n",
    "    root_dir=config[\"val_dataset_path\"],  # NEW\n",
    "    sequence_length=config[\"sequence_length\"],\n",
    "    scale=config[\"scale\"],\n",
    "    mode='test'  # ensure no shuffle\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # NEW: stop if no improvement for 5 epochs\n",
    "trigger_times = 0\n",
    "best_model = None\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = CharbonnierLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        lr = batch[\"lr\"].to(device)\n",
    "        hr = batch[\"hr\"].to(device)\n",
    "\n",
    "        sr = model(lr)\n",
    "        sr = torch.clamp(sr, 0.0, 1.0)\n",
    "        hr = torch.clamp(hr, 0.0, 1.0)\n",
    "        loss = criterion(sr, hr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(loader)\n",
    "    print(f\"[Epoch {epoch+1}] Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            lr = batch[\"lr\"].to(device)\n",
    "            hr = batch[\"hr\"].to(device)\n",
    "            sr = model(lr)\n",
    "            sr = torch.clamp(sr, 0.0, 1.0)\n",
    "            hr = torch.clamp(hr, 0.0, 1.0)\n",
    "            loss = criterion(sr, hr)\n",
    "            val_loss += loss.item()\n",
    "            val_psnr += calc_psnr(sr[0], hr[0])\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_psnr = val_psnr / len(val_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Val Loss: {avg_val_loss:.4f} | Val PSNR: {avg_val_psnr:.2f} dB\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "        best_model = copy.deepcopy(model.state_dict())  # save best model\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Save sample output\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_image(sr[0].cpu(), f\"outputepoch_modified{epoch+1}.png\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "        torch.save(model.state_dict(), f\"weights/modified-model/vsrepoch_modified{epoch+1}.pth\")\n",
    "\n",
    "torch.save(best_model, \"weights/modified-model/vsr_best_model.pth\")  # NEW\n",
    "print(\"✅ Best model saved to vsr_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbeb4ef7-5e70-4468-b0ca-ffb49707cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2400/2400 [04:08<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg PSNR over test set: 24.75 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"data/config.yaml\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "dataset = VSRDataset(\n",
    "    root_dir=config[\"test_dataset_path\"],\n",
    "    sequence_length=config[\"sequence_length\"],\n",
    "    scale=config[\"scale\"],\n",
    "    mode='test'\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = VSRTransformer(\n",
    "    in_channels=3,\n",
    "    feat_channels=config[\"feat_channels\"],\n",
    "    scale=config[\"scale\"],\n",
    "    num_blocks=config[\"num_blocks\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"weights/modified-model/vsr_best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(\"test_outputs\", exist_ok=True)\n",
    "\n",
    "psnr_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(loader, desc=\"Testing\")):\n",
    "        lr = batch[\"lr\"].to(device)\n",
    "        hr = batch[\"hr\"].to(device)\n",
    "\n",
    "        sr = model(lr)\n",
    "        sr = torch.clamp(sr, 0.0, 1.0)\n",
    "        hr = torch.clamp(hr, 0.0, 1.0)\n",
    "\n",
    "        psnr = calc_psnr(sr[0], hr[0])\n",
    "        psnr_total += psnr\n",
    "\n",
    "        save_image(sr[0].cpu(), f\"test_outputs/sr_{i:04d}.png\")\n",
    "        save_image(hr[0].cpu(), f\"test_outputs/hr_{i:04d}.png\")\n",
    "\n",
    "avg_psnr = psnr_total / len(loader)\n",
    "print(f\"\\nAvg PSNR over test set: {avg_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ec27c-8f42-4418-9578-ba9ab13d8aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
